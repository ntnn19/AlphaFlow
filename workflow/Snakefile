import subprocess
import os
DATASET = config["dataset"]
OUTPUT_DIR = config["output_dir"]

INPUTS,  = glob_wildcards(os.path.join(DATASET,"{i}.json"))


rule all:
    input:
        expand(os.path.join(OUTPUT_DIR,"{i}/{i}/{i}_inference.done.txt"),i=INPUTS),


rule RUN_AF3_DATA:
    input:
        os.path.join(DATASET,"{i}.json")
    output:
        os.path.join(OUTPUT_DIR,"{i}/{i}_data_pipeline.done.txt")
    resources:
        slurm_account="cssb",
        slurm_partition="topfgpu",
        nodes=1,
        runtime=10000,
    container:
        config["af3_container"]
    shell:
        """
        python /app/alphafold/run_alphafold.py --json_path=/root/af_input/{input} \
        --model_dir=/root/models \
        --output_dir=/root/af_output \
        --db_dir=/root/public_databases \
        --run_data_pipeline=true \
        --run_inference=false
        """


rule RUN_AF3_INFERENCE:
    input:
        data_pipeline_done_flag= os.path.join(OUTPUT_DIR,"{i}/{i}_data_pipeline.done.txt")
    output:
        touch(os.path.join(OUTPUT_DIR,"{i}/{i}/{i}_inference.done.txt")),
    resources:
        slurm_account="cssb",
        slurm_partition="cssbgpu",
        constraint="A100-SXM4-80GB",
        nodes=1,
        runtime=10000,
        memory="256G",
    container:
        config["af3_container"]
    shell:
        """
        python /app/alphafold/run_alphafold.py --json_path=/root/af_output/{wildcards.i}/{wildcards.i}_data.json \
        --model_dir=/root/models \
        --output_dir=/root/af_output/{wildcards.i} \
        --db_dir=/root/public_databases \
        --run_data_pipeline=false \
        --run_inference=true
        """

