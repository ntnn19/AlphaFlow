import subprocess
import os
DATASET = config["dataset"]
OUTPUT_DIR = config["output_dir"]

INPUTS,  = glob_wildcards(os.path.join(DATASET,"{i}.json"))


rule all:
    input:
        expand(os.path.join(OUTPUT_DIR,"{i}/{i}/{i}_inference.done.txt"),i=INPUTS),


rule RUN_AF3_DATA:
    input:
        os.path.join(DATASET,"{i}.json")
    output:
        touch(os.path.join(OUTPUT_DIR,"{i}/{i}_data_pipeline.done.txt"))
    resources:
        slurm_partition="vds",
        nodes=1,
        runtime=10000,
        cpus_per_task=16,
    container:
        config["af3_container"]
    shell:
        """
        python /app/alphafold/run_alphafold.py --json_path=/root/af_input/{wildcards.i}.json \
        --model_dir=/root/models \
        --output_dir=/root/af_output \
        --db_dir=/root/public_databases \
        --run_data_pipeline=true \
        --run_inference=false
        """


rule RUN_AF3_INFERENCE:
    input:
        data_pipeline_done_flag= os.path.join(OUTPUT_DIR,"{i}/{i}_data_pipeline.done.txt")
    output:
        touch(os.path.join(OUTPUT_DIR,"{i}/{i}/{i}_inference.done.txt")),
    resources:
        slurm_extra="'--gpus-per-node=1'",
#        slurm_extra="'--gres=gpu:1,--gpus-per-node=1'",
        nodes=1,
        runtime=10000,
        memory="256G",
    container:
        config["af3_container"]
    shell:
        """
        python /app/alphafold/run_alphafold.py --json_path=/root/af_output/{wildcards.i}/{wildcards.i}_data.json \
        --model_dir=/root/models \
        --output_dir=/root/af_output/{wildcards.i} \
        --db_dir=/root/public_databases \
        --run_data_pipeline=false \
        --run_inference=true
        """
